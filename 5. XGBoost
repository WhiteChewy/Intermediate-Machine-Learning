Градиентный бустинг деревьев и библиотека XGBoost

В течении этого курса в большинстве своем мы осуществляли предсказания используя метод Случайного Леса (RandomForest), который
неплохо работает по сравнению с одним "Древом Выбора" просто потому что этом методе используется среднее от множества "древ выбора".

Метод "Случайного Леса" относится к "АНСАМБЛЕВЫМ МЕТОДАМ". По определению, ансамблевые методы комбинируют предсказания нескольких
моделей (нескольких деревьев, в случае с случайным лесом)

Далее мы рассмотрим другой Ансамблевый метод который называется "Градиентный бустинг".

ГРАДИЕНТНЫЙ БУСТИНГ
Градиентный бустинг это метод который использует цикл чтобы иттеративно добавить модель в "ансамбль".

Все начинается с инициализации ансамбля одиночной моделью, чьи предсказания могут быть довольно не точны (наивны). (Даже если 
ее предсказания необычайно неточны, последующие добавления в ансамбль устранят эти ошибки)

Далее мы начинаем наш цикл:
- Во-первых, мы используем текущий ансамбль чтобы спрогнозировать каждое наблюдение в датасете. Для того чтобы сделать
предсказание, мы складываем предсказания всех моделей в ансамбле.
- Эти предсказания используются для оценки потерь функции (например Средняя Квадратичная Ошибка)
-Далее мы используем потери функции чтобы подогнать модель, которая далее будет добавленна в ансамбль. Если конкретно, то
мы определяем параметры так чтобы добавление этой новой модели в ансамбль уменьшило потери. (Слово ГРАДИЕНТ в названии 
Градиентный бустинг говорит о том что мы буедем использовать градиентный спуск на потерях функции для того чтобы определить
параметры новой модели для добавления в эту новую модель)
- Добавляем модель в ансамбль ИИИИИИИ
- все по новой!

Пример в файле 5_XGBoost.py

XGBoost

Улучшение параметров
У XGBoost есть несколько параметров которые могут неплохо влиять на точность и скорость обучения. Первые параметры которые
нужно понять это:

n_estimators
Этот параметр функции определяет сколько раз будет повторяться цикл в градиентном бустинге. Так же по совместительству он 
показывает количество используемых моделей в ансамбле.
- Если значение будет слишком низким это вызовет недостаточную подгонку модели, что приведет к неточным предсазания что на
тестовых данных что на проверочных.
- Слишком высокое значение вызывет переизбыточную подгонку модели, что вызовет конечно точные предсказания на тестовых данных,
но не точные предсказания на проверочных.

В основном оптимальное значение лежит в промежутке [100; 1000], собственно это больше зависит от параметра learning_rate
который мы разберем ниже.

learning_rate
Вместо того чтобы получать предсказания просто добавляя предсказания каждого компаненита модели, мы можем умножать предсказания
от каждой модели на небольшое число (которые называется "темп обучения") перед складыванием.

Это означает что каждое древо которое мы будем добавлять в ансамбль будет помогать нам меньше. Именно поэтому мы может установить
более высокое значение в аргумент n_estimators не вызывая черезмерную подгонку модели. Если мы используем "раннюю остановку",
соответствующее количество деревьев будет определено автоматически.

В основном, маленький темп обучения и большое количество статистических оценок будут приводить к более аккуратным XGBoost моделям.
но он так же будет увеличивать время ща которое модель обучается, потому как ей потребуется большее количество итераций.
По умолчанию XGBoost ставит значение темпа обучения равным 0,1

n_jobs
На более емких датасетах, где время выполнения важно, мы можем использовать параллелизирование чтобы быстрее построить модель.
Обычно параметр n_jobs ставят равным количеству ядер процессора на компьютере. На маленьких датасетах это не особо помогает.

Итоговая модель не будет лучше, но это микро-оптимизация.
