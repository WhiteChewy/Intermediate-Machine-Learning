Перекрестная проверка

Машинное обучение - итеративный процесс.

В дальнейшем ты встретишься с выбором какие предикативные перменные использовать, какие типы моделей использовать, какие
аргументы скармливать моделям и т.д. До этого, вы делали эти выборы способом который был управляем данными используя
замер качества моделей с проверочным множеством. 

Но вот вам некоторые недостатки этого способа. Давайте представим что у вас есть датасет с 5000 строк. Обычно вы будете
отделять около 20% датасета как сет для проверки или 1000 строк. Но это оставляет нам случайны шанс в определении "счета"
моделей. Т.е. модель может справляться на множестве из 1000 строк, даже если она будет не точна на других 1000 строках.

В крайнем случае вы можете представить что у вас только 1 строчка данных в проверочном множестве. Если вы сравните альтернативные
модели, то какая модель сделает предсказание точнее зависит только от удачи.

В большинстве своем, чем больше проверочные множества, тем меньше рандом (ака "шум") в нашем измерении качества моделей, и тем более
надежнее они будут. К сожалению, мы можем только получить большое проверочное множество только удаляя строки с наших данных для
проверочных данных, и уменьшать тренировочный датасет, что влечет ухудшение моделей.

ЧТО ТАКОЕ ПЕРЕКРЕСТНАЯ ПРОВЕРКА?
В перекрестной проверке, мы запускаем наш процесс моделирования на разных подмножествах данных для того чтобы получить несколько
замеров качества моделей.

Например, мы можем начать разделив данные на 5 кусков, каждый по 20 процентов от полного датасета. В этом случае, мы говорим о
том что мы разбиваем данные на 5 "складок".
                  |<------------------------ Total Dataset ----------------------->|
                  +------------+------------+------------+------------+------------+
    Experiment 1  | Validation |  Training  |  Training  |  Training  |  Training  |
                  +------------+------------+------------+------------+------------+
    Experiment 2  |  Training  | Validation |  Training  |  Training  |  Training  |
                  +------------+------------+------------+------------+------------+
    Experiment 3  |  Training  |  Training  | Validation |  Training  |  Training  |
                  +------------+------------+------------+------------+------------+
    Experiment 4  |  Training  |  Training  |  Training  | Validation |  Training  |
                  +------------+------------+------------+------------+------------+
    Experiment 5  |  Training  |  Training  |  Training  |  Training  | Validation |
                  +------------+------------+------------+------------+------------+
                  |<-1st fold->|<-2nd fold->|<-3rd fold->|<-4th fold->|<-5th fold->|

Далее мы запускаем 1 эксперимент на каждую складку:
- В эксперименте 1 мы используем первую складку как проверочную и все остальное как тренировочные данные. Это дает нам измерение
качества модели основанной на 20% выдержке сета.
- В эксперименте 2 мы используем вторую складку как првоерочную  и все остальное как тренировочные данные. Этот дает нам вторую
оценку качества моделей.
- Мы повторяем этот процесс используя каждую складку по одному разу как проверочный сет. Суммируя, 100% данных используется как
выдержка в некотором роде, и в конце концов приходим к тому что измерение качества модели основанно на всех строках в датасете.

Когда лучше использовать перекрестную проверку?
Перекрестное обучение дает нам более точное измерение качества моделей, что в частности важно если вы делаете множество выборов
в процессе моделлинга. Однако, это может затратить больше времени, потому что происходит оценка нескольких моделей (одна на 
каждую складку). 

И так, исходя из данных компромисов, когда мы должны использовать данные подходы?

- Для маленьких датасетов, где дополнительные затраты на вычисления не очень большие, неплохо будет использовать
перекрестную проверку.
- Для больших датасетов, единственного проверочного сета будет достаточно.

Пример в файле 4_Cross_Validation.py